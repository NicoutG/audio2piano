{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(\"src\").resolve()))\n",
    "\n",
    "from audio2piano import Audio2Piano, note_matrices_to_notes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = Audio2Piano(\"weights/model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_spectrogram(mat, title, y_label):\n",
    "    if mat.shape[0] < mat.shape[1]:\n",
    "        mat = mat.T\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.imshow(mat.T, aspect=\"auto\", origin=\"lower\", cmap=\"magma\")\n",
    "    plt.xlabel(\"Time frames\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def show_piano_roll(mat, title=\"Piano Roll\"):\n",
    "    if mat.shape[1] != 88:\n",
    "        mat = mat.T\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.imshow(mat.T, aspect=\"auto\", origin=\"lower\", cmap=\"gray_r\")\n",
    "    plt.xlabel(\"Time frames\")\n",
    "    plt.ylabel(\"MIDI pitch (21â€“108)\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def midi_to_sustain_roll(notes, total_steps, hop_sec=0.05):\n",
    "    roll = np.zeros((88, total_steps), dtype=np.float32)\n",
    "\n",
    "    for note in notes:\n",
    "        pitch = note[\"pitch\"] - 21\n",
    "        if not (0 <= pitch < 88):\n",
    "            continue\n",
    "\n",
    "        start = int(note[\"start\"] / hop_sec)\n",
    "        end   = int(note[\"end\"]   / hop_sec)\n",
    "\n",
    "        if start >= total_steps:\n",
    "            continue\n",
    "\n",
    "        end = max(start + 1, end)\n",
    "        end = min(end, total_steps)\n",
    "\n",
    "        duration = end - start\n",
    "\n",
    "        for i, t in enumerate(range(start, end)):\n",
    "            alpha = i / duration\n",
    "\n",
    "            value = 1.0 - 0.5 * alpha\n",
    "\n",
    "            roll[pitch, t] = max(roll[pitch, t], value)\n",
    "\n",
    "    return roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_FILE = \"data/musics/wav(input)/example2.wav\"\n",
    "THRESHOLD = 0.6\n",
    "\n",
    "samples, sr = transcriber.load_wav(WAV_FILE)\n",
    "mels = transcriber.wav_to_mel(samples, sr)\n",
    "\n",
    "show_spectrogram(mels, \"Mel Spectrogram\", \"Mel bins\")\n",
    "\n",
    "\n",
    "x = mels.T.detach().unsqueeze(0).float().to(transcriber.device)\n",
    "onset_logits, sustain_logits = transcriber.forward(x)\n",
    "onset_probs = torch.sigmoid(onset_logits)[0].cpu().detach().numpy()\n",
    "sustain_probs = torch.sigmoid(sustain_logits)[0].cpu().detach().numpy()\n",
    "\n",
    "show_piano_roll(onset_probs, \"Piano roll (Onset)\")\n",
    "show_piano_roll(sustain_probs, \"Piano roll (Sustain)\")\n",
    "\n",
    "\n",
    "duration = len(samples) / sr\n",
    "total_steps = int(duration / 0.05) + 1\n",
    "notes = note_matrices_to_notes(onset_matrix=onset_probs, sustain_matrix=sustain_probs, onset_threshold=THRESHOLD)\n",
    "midi_matrix = midi_to_sustain_roll(notes, total_steps=total_steps)\n",
    "\n",
    "show_piano_roll(midi_matrix, \"Piano roll (MIDI final)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_FILE = \"data/musics/wav(input)/example0.wav\"\n",
    "OUTPUT_FILE = \"data/musics/midi(output)/example0.mid\"\n",
    "\n",
    "midi = transcriber.wav_to_midi_file(WAV_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081dc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_FOLDER = \"data/musics/wav(input)\"\n",
    "OUTPUT_FOLDER = \"data/musics/midi(output)\"\n",
    "\n",
    "midi = transcriber.wav_to_midi_folder(WAV_FOLDER, OUTPUT_FOLDER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
